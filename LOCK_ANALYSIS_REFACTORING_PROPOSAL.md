# é”åˆ†ææ¨¡å—é‡æ„ä¸ä¼˜åŒ–æ–¹æ¡ˆ

## ğŸ“‹ ç›®å½•
1. [ç°çŠ¶åˆ†æ](#ç°çŠ¶åˆ†æ)
2. [ä¸šç•Œæœ€ä½³å®è·µ](#ä¸šç•Œæœ€ä½³å®è·µ)
3. [é‡æ„ç›®æ ‡](#é‡æ„ç›®æ ‡)
4. [æ¶æ„è®¾è®¡](#æ¶æ„è®¾è®¡)
5. [è¯¦ç»†è®¾è®¡æ–¹æ¡ˆ](#è¯¦ç»†è®¾è®¡æ–¹æ¡ˆ)
6. [å®æ–½è®¡åˆ’](#å®æ–½è®¡åˆ’)
7. [é¢„æœŸæ”¶ç›Š](#é¢„æœŸæ”¶ç›Š)

---

## 1. ç°çŠ¶åˆ†æ

### 1.1 å½“å‰æ¶æ„ä¼˜ç‚¹
âœ… **å®Œæ•´çš„åŠŸèƒ½è¦†ç›–**: å®æ—¶åˆ†æã€å†å²åˆ†æã€ç­‰å¾…é“¾æ£€æµ‹ã€ç«äº‰åˆ†æ  
âœ… **å¤šæ•°æ®åº“æ”¯æŒ**: PostgreSQLã€MySQLã€OceanBase  
âœ… **åˆ†å±‚æ¶æ„**: Modelã€Serviceã€APIã€Frontend åˆ†å±‚æ¸…æ™°  
âœ… **æ•°æ®æ¨¡å‹è®¾è®¡**: å®Œæ•´çš„æ•°æ®åº“è¡¨ç»“æ„è®¾è®¡  

### 1.2 å½“å‰æ¶æ„é—®é¢˜

#### ğŸ”´ æ¶æ„å±‚é¢
1. **Mockæ•°æ®ç¡¬ç¼–ç **: `lock_analyzer_providers.py` ä½¿ç”¨ç¡¬ç¼–ç çš„æ¨¡æ‹Ÿæ•°æ®ï¼Œç¼ºä¹çœŸå®æ•°æ®é‡‡é›†
2. **ç´§è€¦åˆè®¾è®¡**: `LockAnalyzer` ç±»èŒè´£è¿‡å¤šï¼Œè¿åå•ä¸€èŒè´£åŸåˆ™ï¼ˆSRPï¼‰
3. **åŒæ­¥ä¼šè¯æ··ç”¨**: APIå±‚æ··ç”¨å¼‚æ­¥å’ŒåŒæ­¥ä¼šè¯ï¼Œå­˜åœ¨çº¿ç¨‹å®‰å…¨éšæ‚£
4. **ç¼ºä¹ç­–ç•¥æ¨¡å¼**: ä¸åŒæ•°æ®åº“ç±»å‹çš„å¤„ç†é€»è¾‘æœªå……åˆ†æŠ½è±¡

#### ğŸŸ¡ æ€§èƒ½å±‚é¢
1. **æ— ç¼“å­˜æœºåˆ¶**: é¢‘ç¹æŸ¥è¯¢é”ä¿¡æ¯æ— ç¼“å­˜ä¼˜åŒ–
2. **N+1æŸ¥è¯¢é—®é¢˜**: æ‰¹é‡æ•°æ®æŸ¥è¯¢å­˜åœ¨æ€§èƒ½ç“¶é¢ˆ
3. **æ— å¼‚æ­¥ä¼˜åŒ–**: å¤§éƒ¨åˆ†æ•°æ®åº“æŸ¥è¯¢æœªä½¿ç”¨å¼‚æ­¥æ¨¡å¼
4. **ç¼ºä¹è¿æ¥æ± ç®¡ç†**: ç›®æ ‡æ•°æ®åº“è¿æ¥æœªç»Ÿä¸€ç®¡ç†

#### ğŸŸ¢ åŠŸèƒ½å±‚é¢
1. **åˆ†æç®—æ³•ç®€å•**: å¥åº·è¯„åˆ†ã€ç«äº‰æ¨¡å¼è¯†åˆ«ç®—æ³•è¿‡äºç®€å•
2. **ç¼ºä¹æœºå™¨å­¦ä¹ **: æ— é¢„æµ‹æ€§åˆ†æèƒ½åŠ›
3. **ä¼˜åŒ–å»ºè®®å›ºå®š**: å»ºè®®ç”Ÿæˆé€»è¾‘ä¸å¤Ÿæ™ºèƒ½å’ŒåŠ¨æ€
4. **ç›‘æ§ä¸å®Œå–„**: å®æ—¶ç›‘æ§ã€å‘Šè­¦æœºåˆ¶æœªå®ç°

#### ğŸŸ£ ä»£ç è´¨é‡å±‚é¢
1. **æµ‹è¯•è¦†ç›–ä¸è¶³**: ç¼ºå°‘å•å…ƒæµ‹è¯•å’Œé›†æˆæµ‹è¯•
2. **é”™è¯¯å¤„ç†ç²—ç³™**: å¼‚å¸¸å¤„ç†ä¸å¤Ÿç»†è‡´ï¼Œç¼ºä¹é‡è¯•æœºåˆ¶
3. **æ—¥å¿—ä¸å®Œå–„**: ç¼ºä¹ç»“æ„åŒ–æ—¥å¿—å’Œè°ƒè¯•ä¿¡æ¯
4. **æ–‡æ¡£ç¼ºå¤±**: ä»£ç æ³¨é‡Šå’ŒAPIæ–‡æ¡£ä¸å®Œæ•´

---

## 2. ä¸šç•Œæœ€ä½³å®è·µ

### 2.1 å‚è€ƒæ ‡æ†äº§å“

#### ğŸ† **Percona Monitoring and Management (PMM)**
- **æ¶æ„ç‰¹ç‚¹**: 
  - Prometheus + Grafana ç›‘æ§æ ˆ
  - VictoriaMetrics æ—¶åºæ•°æ®åº“
  - ClickHouse åˆ†æå¼•æ“
- **æ ¸å¿ƒèƒ½åŠ›**:
  - å®æ—¶æ€§èƒ½ç›‘æ§å’Œå¯è§†åŒ–
  - Query Analytics æ…¢æŸ¥è¯¢åˆ†æ
  - æ™ºèƒ½å‘Šè­¦å’Œå¼‚å¸¸æ£€æµ‹

#### ğŸ† **DataDog Database Monitoring**
- **æ¶æ„ç‰¹ç‚¹**:
  - Agent-based æ•°æ®é‡‡é›†
  - äº‘åŸç”Ÿåˆ†å¸ƒå¼æ¶æ„
  - æœºå™¨å­¦ä¹ é©±åŠ¨çš„å¼‚å¸¸æ£€æµ‹
- **æ ¸å¿ƒèƒ½åŠ›**:
  - å®æ—¶é”ç­‰å¾…å¯è§†åŒ–
  - å†å²è¶‹åŠ¿åˆ†æå’Œå¯¹æ¯”
  - è‡ªåŠ¨åŒ–æ ¹å› åˆ†æ

#### ğŸ† **AWS RDS Performance Insights**
- **æ¶æ„ç‰¹ç‚¹**:
  - ä½å¼€é”€æ•°æ®é‡‡é›† (<1% CPU)
  - æ—¶åºæ•°æ®å‹ç¼©å­˜å‚¨
  - SQLçº§åˆ«çš„æ€§èƒ½åˆ†æ
- **æ ¸å¿ƒèƒ½åŠ›**:
  - Top SQL by Wait Time
  - Database Load å¯è§†åŒ–
  - ç»†ç²’åº¦è¿‡æ»¤å’Œé’»å–

### 2.2 æ ¸å¿ƒè®¾è®¡åŸåˆ™

#### SOLID åŸåˆ™
- **å•ä¸€èŒè´£åŸåˆ™ (SRP)**: æ¯ä¸ªç±»åªè´Ÿè´£ä¸€ä¸ªåŠŸèƒ½é¢†åŸŸ
- **å¼€é—­åŸåˆ™ (OCP)**: å¯¹æ‰©å±•å¼€æ”¾ï¼Œå¯¹ä¿®æ”¹å…³é—­
- **é‡Œæ°æ›¿æ¢åŸåˆ™ (LSP)**: å­ç±»èƒ½å¤Ÿæ›¿æ¢çˆ¶ç±»
- **æ¥å£éš”ç¦»åŸåˆ™ (ISP)**: æ¥å£åº”è¯¥å°è€Œä¸“æ³¨
- **ä¾èµ–å€’ç½®åŸåˆ™ (DIP)**: ä¾èµ–æŠ½è±¡è€Œéå…·ä½“å®ç°

#### è®¾è®¡æ¨¡å¼åº”ç”¨
- **ç­–ç•¥æ¨¡å¼**: ä¸åŒæ•°æ®åº“çš„é”åˆ†æç­–ç•¥
- **å·¥å‚æ¨¡å¼**: åˆ†æå™¨å’Œé‡‡é›†å™¨çš„åˆ›å»º
- **è§‚å¯Ÿè€…æ¨¡å¼**: å®æ—¶ç›‘æ§å’Œå‘Šè­¦
- **è´£ä»»é“¾æ¨¡å¼**: åˆ†æå»ºè®®ç”Ÿæˆæµç¨‹
- **è£…é¥°å™¨æ¨¡å¼**: ç¼“å­˜ã€æ—¥å¿—ã€æ€§èƒ½ç›‘æ§

#### æ€§èƒ½ä¼˜åŒ–æœ€ä½³å®è·µ
- **å¼‚æ­¥IO**: ä½¿ç”¨ asyncio å¤„ç†å¹¶å‘è¯·æ±‚
- **è¿æ¥æ± **: å¤ç”¨æ•°æ®åº“è¿æ¥å‡å°‘å¼€é”€
- **ç¼“å­˜ç­–ç•¥**: Redis + å¤šçº§ç¼“å­˜
- **æ‰¹é‡æ“ä½œ**: å‡å°‘æ•°æ®åº“å¾€è¿”æ¬¡æ•°
- **ç´¢å¼•ä¼˜åŒ–**: ç¡®ä¿æŸ¥è¯¢æœ‰åˆé€‚çš„ç´¢å¼•

---

## 3. é‡æ„ç›®æ ‡

### 3.1 ä¸šåŠ¡ç›®æ ‡
ğŸ¯ **æå‡å‡†ç¡®æ€§**: ä»Mockæ•°æ®åˆ°çœŸå®æ•°æ®é‡‡é›†ï¼Œå‡†ç¡®ç‡æå‡è‡³95%+  
ğŸ¯ **é™ä½å»¶è¿Ÿ**: APIå“åº”æ—¶é—´ä»500msé™è‡³100msä»¥å†…  
ğŸ¯ **æå‡å¯æ‰©å±•æ€§**: æ”¯æŒ5åˆ†é’Ÿå†…æ¥å…¥æ–°æ•°æ®åº“ç±»å‹  
ğŸ¯ **å¢å¼ºæ™ºèƒ½åŒ–**: å¼•å…¥æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œé¢„æµ‹å‡†ç¡®ç‡è¾¾åˆ°80%+  

### 3.2 æŠ€æœ¯ç›®æ ‡
ğŸ”§ **ä»£ç è´¨é‡**: æµ‹è¯•è¦†ç›–ç‡è¾¾åˆ°80%+ï¼ŒSonaræ‰«ææ— é‡å¤§é—®é¢˜  
ğŸ”§ **æ€§èƒ½æŒ‡æ ‡**: æ”¯æŒ10000+ TPSï¼ŒP99å»¶è¿Ÿ<200ms  
ğŸ”§ **å¯ç»´æŠ¤æ€§**: ä»£ç å¤æ‚åº¦é™ä½30%ï¼Œæ–°äººä¸Šæ‰‹æ—¶é—´ç¼©çŸ­50%  
ğŸ”§ **å¯é æ€§**: ç³»ç»Ÿå¯ç”¨æ€§è¾¾åˆ°99.9%ï¼Œæ•…éšœæ¢å¤æ—¶é—´<5åˆ†é’Ÿ  

---

## 4. æ¶æ„è®¾è®¡

### 4.1 æ•´ä½“æ¶æ„å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         Frontend Layer                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚ Dashboard    â”‚  â”‚ Analysis     â”‚  â”‚ Optimization â”‚          â”‚
â”‚  â”‚ Component    â”‚  â”‚ Component    â”‚  â”‚ Component    â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚ HTTP/REST API
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         API Gateway Layer                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚ Auth/Rate    â”‚  â”‚ Request      â”‚  â”‚ Response     â”‚          â”‚
â”‚  â”‚ Limiting     â”‚  â”‚ Validation   â”‚  â”‚ Cache        â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Business Service Layer                        â”‚
â”‚                                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚         Lock Analysis Orchestrator                  â”‚        â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚        â”‚
â”‚  â”‚  â”‚ Real-timeâ”‚  â”‚Historicalâ”‚  â”‚Predictiveâ”‚          â”‚        â”‚
â”‚  â”‚  â”‚ Analyzer â”‚  â”‚ Analyzer â”‚  â”‚ Analyzer â”‚          â”‚        â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                            â”‚                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚                                                    â”‚          â”‚
â”‚  â–¼                        â–¼                          â–¼          â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚ â”‚ Lock Data    â”‚  â”‚ Analysis     â”‚  â”‚ Optimization â”‚          â”‚
â”‚ â”‚ Collector    â”‚  â”‚ Engine       â”‚  â”‚ Advisor      â”‚          â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Data Access Layer                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚ PostgreSQL   â”‚  â”‚ MySQL        â”‚  â”‚ OceanBase    â”‚          â”‚
â”‚  â”‚ Provider     â”‚  â”‚ Provider     â”‚  â”‚ Provider     â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Infrastructure Layer                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚ Redis Cache  â”‚  â”‚ Time-Series  â”‚  â”‚ Message      â”‚          â”‚
â”‚  â”‚              â”‚  â”‚ Database     â”‚  â”‚ Queue        â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 4.2 æ ¸å¿ƒæ¨¡å—è®¾è®¡

#### 4.2.1 æ•°æ®é‡‡é›†å±‚ (Data Collector)

```python
# æ¥å£å®šä¹‰
class ILockDataCollector(ABC):
    """é”æ•°æ®é‡‡é›†å™¨æ¥å£"""
    
    @abstractmethod
    async def collect_current_locks(self) -> List[LockSnapshot]:
        """é‡‡é›†å½“å‰é”çŠ¶æ€"""
        pass
    
    @abstractmethod
    async def collect_wait_chains(self) -> List[WaitChain]:
        """é‡‡é›†é”ç­‰å¾…é“¾"""
        pass
    
    @abstractmethod
    async def collect_lock_statistics(self, duration: timedelta) -> LockStatistics:
        """é‡‡é›†é”ç»Ÿè®¡ä¿¡æ¯"""
        pass

# å®ç°ç±»
class PostgreSQLLockCollector(ILockDataCollector):
    """PostgreSQLé”æ•°æ®é‡‡é›†å™¨"""
    
    async def collect_current_locks(self) -> List[LockSnapshot]:
        # æŸ¥è¯¢ pg_locks è§†å›¾
        query = """
        SELECT 
            locktype, database, relation, page, tuple, virtualxid, 
            transactionid, classid, objid, objsubid, virtualtransaction,
            pid, mode, granted, fastpath
        FROM pg_locks
        WHERE NOT granted OR locktype IN ('relation', 'transactionid')
        """
        # æ‰§è¡ŒæŸ¥è¯¢å¹¶è§£æç»“æœ
```

#### 4.2.2 åˆ†æå¼•æ“å±‚ (Analysis Engine)

```python
class LockAnalysisEngine:
    """é”åˆ†æå¼•æ“ - è´£ä»»é“¾æ¨¡å¼"""
    
    def __init__(self):
        self.analyzers: List[IAnalyzer] = [
            WaitChainAnalyzer(),
            ContentionAnalyzer(),
            DeadlockAnalyzer(),
            PerformanceImpactAnalyzer()
        ]
    
    async def analyze(self, data: LockData) -> AnalysisResult:
        """æ‰§è¡Œå®Œæ•´çš„é”åˆ†æ"""
        results = []
        for analyzer in self.analyzers:
            result = await analyzer.analyze(data)
            results.append(result)
        
        return self._aggregate_results(results)

class WaitChainAnalyzer(IAnalyzer):
    """ç­‰å¾…é“¾åˆ†æå™¨"""
    
    async def analyze(self, data: LockData) -> WaitChainAnalysisResult:
        # æ„å»ºç­‰å¾…å›¾
        graph = self._build_wait_graph(data.locks)
        
        # æ£€æµ‹ç¯è·¯ï¼ˆæ­»é”ï¼‰
        cycles = self._detect_cycles(graph)
        
        # è®¡ç®—ç­‰å¾…é“¾
        chains = self._extract_chains(graph)
        
        # è¯„ä¼°ä¸¥é‡ç¨‹åº¦
        severity = self._assess_severity(chains)
        
        return WaitChainAnalysisResult(
            chains=chains,
            cycles=cycles,
            severity=severity
        )
```

#### 4.2.3 ä¼˜åŒ–å»ºè®®ç”Ÿæˆå™¨ (Optimization Advisor)

```python
class OptimizationAdvisor:
    """ä¼˜åŒ–å»ºè®®ç”Ÿæˆå™¨ - ç­–ç•¥æ¨¡å¼"""
    
    def __init__(self):
        self.strategies: Dict[str, IOptimizationStrategy] = {
            'index': IndexOptimizationStrategy(),
            'query': QueryOptimizationStrategy(),
            'isolation': IsolationLevelStrategy(),
            'config': ConfigurationStrategy(),
            'schema': SchemaDesignStrategy()
        }
    
    async def generate_advice(
        self, 
        analysis: AnalysisResult,
        context: AnalysisContext
    ) -> List[OptimizationAdvice]:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        
        advice_list = []
        
        for strategy_name, strategy in self.strategies.items():
            if strategy.is_applicable(analysis, context):
                advice = await strategy.generate(analysis, context)
                advice_list.extend(advice)
        
        # æŒ‰ä¼˜å…ˆçº§å’Œå½±å“æ’åº
        return self._prioritize_advice(advice_list)

class IndexOptimizationStrategy(IOptimizationStrategy):
    """ç´¢å¼•ä¼˜åŒ–ç­–ç•¥"""
    
    async def generate(
        self, 
        analysis: AnalysisResult,
        context: AnalysisContext
    ) -> List[OptimizationAdvice]:
        """ç”Ÿæˆç´¢å¼•ä¼˜åŒ–å»ºè®®"""
        
        advice = []
        
        # åˆ†æçƒ­ç‚¹è¡¨
        for hot_object in analysis.hot_objects:
            # æ£€æŸ¥æ˜¯å¦æœ‰åˆé€‚çš„ç´¢å¼•
            missing_indexes = await self._check_missing_indexes(
                hot_object, context
            )
            
            if missing_indexes:
                advice.append(OptimizationAdvice(
                    type='index',
                    priority='high',
                    object=hot_object.name,
                    recommendation=f"åˆ›å»ºç´¢å¼•: {missing_indexes}",
                    impact_score=self._calculate_impact(hot_object),
                    sql_script=self._generate_index_sql(missing_indexes)
                ))
        
        return advice
```

### 4.3 æ•°æ®æµè®¾è®¡

```
1. æ•°æ®é‡‡é›†æµ
   Target DB â†’ Collector â†’ Raw Data â†’ Parser â†’ Structured Data

2. åˆ†ææµ
   Structured Data â†’ Analysis Engine â†’ Analysis Result â†’ Cache

3. å»ºè®®ç”Ÿæˆæµ
   Analysis Result â†’ Optimization Advisor â†’ Advice List â†’ Storage

4. å®æ—¶ç›‘æ§æµ
   Collector â†’ Event Stream â†’ Threshold Check â†’ Alert Service
```

---

## 5. è¯¦ç»†è®¾è®¡æ–¹æ¡ˆ

### 5.1 æ•°æ®é‡‡é›†æ¨¡å—é‡æ„

#### 5.1.1 PostgreSQL çœŸå®æ•°æ®é‡‡é›†

```python
class PostgreSQLLockCollector(ILockDataCollector):
    """PostgreSQL é”æ•°æ®é‡‡é›†å™¨"""
    
    def __init__(self, connection_pool: asyncpg.Pool):
        self.pool = connection_pool
        self.queries = PostgreSQLLockQueries()
    
    async def collect_current_locks(self) -> List[LockSnapshot]:
        """é‡‡é›†å½“å‰é”çŠ¶æ€"""
        async with self.pool.acquire() as conn:
            rows = await conn.fetch(self.queries.CURRENT_LOCKS)
            return [self._parse_lock_row(row) for row in rows]
    
    async def collect_blocking_tree(self) -> List[BlockingTree]:
        """é‡‡é›†é˜»å¡æ ‘"""
        async with self.pool.acquire() as conn:
            # ä½¿ç”¨é€’å½’CTEæŸ¥è¯¢é˜»å¡å…³ç³»
            query = """
            WITH RECURSIVE blocking_tree AS (
                -- åŸºç¡€æŸ¥è¯¢ï¼šæ‰¾åˆ°æ‰€æœ‰è¢«é˜»å¡çš„è¿›ç¨‹
                SELECT 
                    blocked.pid AS blocked_pid,
                    blocked.query AS blocked_query,
                    blocking.pid AS blocking_pid,
                    blocking.query AS blocking_query,
                    1 AS level,
                    ARRAY[blocked.pid] AS chain
                FROM pg_stat_activity AS blocked
                JOIN pg_locks AS blocked_locks ON blocked.pid = blocked_locks.pid
                JOIN pg_locks AS blocking_locks ON 
                    blocked_locks.locktype = blocking_locks.locktype
                    AND blocked_locks.database = blocking_locks.database
                    AND blocked_locks.relation = blocking_locks.relation
                    AND blocked_locks.granted = false
                    AND blocking_locks.granted = true
                JOIN pg_stat_activity AS blocking ON blocking_locks.pid = blocking.pid
                WHERE blocked.wait_event_type = 'Lock'
                
                UNION ALL
                
                -- é€’å½’æŸ¥è¯¢ï¼šç»§ç»­è¿½è¸ªé˜»å¡é“¾
                SELECT 
                    bt.blocked_pid,
                    bt.blocked_query,
                    next_blocking.pid,
                    next_blocking.query,
                    bt.level + 1,
                    bt.chain || next_blocking.pid
                FROM blocking_tree bt
                JOIN pg_stat_activity AS next_blocked ON bt.blocking_pid = next_blocked.pid
                JOIN pg_locks AS next_blocked_locks ON next_blocked.pid = next_blocked_locks.pid
                JOIN pg_locks AS next_blocking_locks ON 
                    next_blocked_locks.locktype = next_blocking_locks.locktype
                    AND next_blocked_locks.database = next_blocking_locks.database
                    AND next_blocked_locks.relation = next_blocking_locks.relation
                    AND next_blocked_locks.granted = false
                    AND next_blocking_locks.granted = true
                JOIN pg_stat_activity AS next_blocking ON next_blocking_locks.pid = next_blocking.pid
                WHERE next_blocked.wait_event_type = 'Lock'
                    AND NOT next_blocking.pid = ANY(bt.chain)  -- é˜²æ­¢ç¯è·¯
                    AND bt.level < 10  -- é™åˆ¶é€’å½’æ·±åº¦
            )
            SELECT * FROM blocking_tree
            ORDER BY level, blocked_pid;
            """
            
            rows = await conn.fetch(query)
            return self._build_blocking_tree(rows)
    
    async def collect_lock_statistics(
        self, 
        duration: timedelta
    ) -> LockStatistics:
        """é‡‡é›†é”ç»Ÿè®¡ä¿¡æ¯"""
        async with self.pool.acquire() as conn:
            # æŸ¥è¯¢ pg_stat_database è·å–æ­»é”ä¿¡æ¯
            deadlock_query = """
            SELECT deadlocks 
            FROM pg_stat_database 
            WHERE datname = current_database()
            """
            deadlocks = await conn.fetchval(deadlock_query)
            
            # æŸ¥è¯¢ pg_locks è·å–é”åˆ†å¸ƒ
            lock_distribution_query = """
            SELECT 
                locktype,
                mode,
                COUNT(*) as count,
                COUNT(*) FILTER (WHERE NOT granted) as waiting_count
            FROM pg_locks
            GROUP BY locktype, mode
            """
            distribution = await conn.fetch(lock_distribution_query)
            
            return LockStatistics(
                deadlocks=deadlocks,
                lock_distribution=dict(distribution),
                collection_time=datetime.now()
            )
```

#### 5.1.2 MySQL çœŸå®æ•°æ®é‡‡é›†

```python
class MySQLLockCollector(ILockDataCollector):
    """MySQL é”æ•°æ®é‡‡é›†å™¨"""
    
    async def collect_current_locks(self) -> List[LockSnapshot]:
        """é‡‡é›†å½“å‰InnoDBé”çŠ¶æ€"""
        # MySQL 8.0+ ä½¿ç”¨ performance_schema
        query = """
        SELECT 
            t.trx_id,
            t.trx_state,
            t.trx_started,
            t.trx_query,
            l.lock_type,
            l.lock_mode,
            l.lock_status,
            l.lock_data,
            CONCAT(t.trx_mysql_thread_id) as thread_id
        FROM information_schema.innodb_trx t
        LEFT JOIN performance_schema.data_locks l 
            ON t.trx_id = l.engine_transaction_id
        WHERE t.trx_state != 'COMMITTED'
        """
        
        async with self.pool.acquire() as conn:
            rows = await conn.fetch(query)
            return [self._parse_innodb_lock(row) for row in rows]
    
    async def collect_lock_waits(self) -> List[LockWait]:
        """é‡‡é›†InnoDBé”ç­‰å¾…"""
        query = """
        SELECT 
            r.trx_id AS requesting_trx_id,
            r.trx_mysql_thread_id AS requesting_thread,
            r.trx_query AS requesting_query,
            b.trx_id AS blocking_trx_id,
            b.trx_mysql_thread_id AS blocking_thread,
            b.trx_query AS blocking_query,
            w.requesting_lock_id,
            w.blocking_lock_id,
            TIMESTAMPDIFF(SECOND, r.trx_wait_started, NOW()) AS wait_time
        FROM information_schema.innodb_lock_waits w
        JOIN information_schema.innodb_trx r ON w.requesting_trx_id = r.trx_id
        JOIN information_schema.innodb_trx b ON w.blocking_trx_id = b.trx_id
        """
        
        async with self.pool.acquire() as conn:
            rows = await conn.fetch(query)
            return [self._parse_lock_wait(row) for row in rows]
```

### 5.2 åˆ†æå¼•æ“é‡æ„

#### 5.2.1 æ™ºèƒ½å¥åº·è¯„åˆ†ç®—æ³•

```python
class LockHealthScorer:
    """é”å¥åº·è¯„åˆ†å™¨ - ä½¿ç”¨åŠ æƒè¯„åˆ†æ¨¡å‹"""
    
    WEIGHTS = {
        'wait_time': 0.30,        # ç­‰å¾…æ—¶é—´æƒé‡
        'contention': 0.25,       # ç«äº‰ç¨‹åº¦æƒé‡
        'deadlock': 0.20,         # æ­»é”é¢‘ç‡æƒé‡
        'blocking_chain': 0.15,   # é˜»å¡é“¾é•¿åº¦æƒé‡
        'timeout': 0.10           # è¶…æ—¶é¢‘ç‡æƒé‡
    }
    
    def calculate_score(self, metrics: LockMetrics) -> float:
        """è®¡ç®—ç»¼åˆå¥åº·è¯„åˆ† (0-100)"""
        
        # 1. ç­‰å¾…æ—¶é—´è¯„åˆ† (åŸºäºP99å»¶è¿Ÿ)
        wait_time_score = self._score_wait_time(
            metrics.avg_wait_time,
            metrics.p99_wait_time,
            metrics.max_wait_time
        )
        
        # 2. ç«äº‰è¯„åˆ† (åŸºäºç«äº‰é¢‘ç‡å’Œå½±å“èŒƒå›´)
        contention_score = self._score_contention(
            metrics.contention_count,
            metrics.affected_sessions,
            metrics.hot_object_count
        )
        
        # 3. æ­»é”è¯„åˆ† (åŸºäºæ­»é”é¢‘ç‡å’Œè§£å†³æ—¶é—´)
        deadlock_score = self._score_deadlocks(
            metrics.deadlock_count,
            metrics.avg_deadlock_resolution_time
        )
        
        # 4. é˜»å¡é“¾è¯„åˆ† (åŸºäºé“¾é•¿åº¦å’ŒæŒç»­æ—¶é—´)
        blocking_chain_score = self._score_blocking_chains(
            metrics.max_chain_length,
            metrics.avg_chain_length,
            metrics.active_chains
        )
        
        # 5. è¶…æ—¶è¯„åˆ† (åŸºäºè¶…æ—¶é¢‘ç‡)
        timeout_score = self._score_timeouts(
            metrics.timeout_count,
            metrics.timeout_rate
        )
        
        # åŠ æƒå¹³å‡
        final_score = (
            wait_time_score * self.WEIGHTS['wait_time'] +
            contention_score * self.WEIGHTS['contention'] +
            deadlock_score * self.WEIGHTS['deadlock'] +
            blocking_chain_score * self.WEIGHTS['blocking_chain'] +
            timeout_score * self.WEIGHTS['timeout']
        )
        
        return max(0.0, min(100.0, final_score))
    
    def _score_wait_time(
        self, 
        avg: float, 
        p99: float, 
        max_time: float
    ) -> float:
        """ç­‰å¾…æ—¶é—´è¯„åˆ†ç®—æ³•"""
        # ä½¿ç”¨é€†Sæ›²çº¿æ˜ å°„
        # ä¼˜ç§€: <100ms -> 90-100åˆ†
        # è‰¯å¥½: 100ms-500ms -> 70-90åˆ†
        # ä¸€èˆ¬: 500ms-2s -> 50-70åˆ†
        # è¾ƒå·®: 2s-5s -> 30-50åˆ†
        # å¾ˆå·®: >5s -> 0-30åˆ†
        
        if p99 < 0.1:  # <100ms
            return 95 + (0.1 - p99) * 50
        elif p99 < 0.5:  # 100-500ms
            return 70 + (0.5 - p99) / 0.4 * 25
        elif p99 < 2.0:  # 500ms-2s
            return 50 + (2.0 - p99) / 1.5 * 20
        elif p99 < 5.0:  # 2s-5s
            return 30 + (5.0 - p99) / 3.0 * 20
        else:  # >5s
            return max(0, 30 - (p99 - 5.0) * 5)
```

#### 5.2.2 ç«äº‰æ¨¡å¼è¯†åˆ«ç®—æ³•

```python
class ContentionPatternRecognizer:
    """ç«äº‰æ¨¡å¼è¯†åˆ«å™¨ - ä½¿ç”¨æœºå™¨å­¦ä¹ åˆ†ç±»"""
    
    PATTERNS = [
        'hot_spot',           # çƒ­ç‚¹ç«äº‰
        'sequential_key',     # é¡ºåºé”®ç«äº‰
        'burst',              # çªå‘ç«äº‰
        'periodic',           # å‘¨æœŸæ€§ç«äº‰
        'cascading',          # çº§è”ç«äº‰
        'deadlock_prone'      # æ˜“æ­»é”æ¨¡å¼
    ]
    
    def recognize_pattern(
        self, 
        contention: ContentionData,
        historical: List[ContentionData]
    ) -> ContentionPattern:
        """è¯†åˆ«ç«äº‰æ¨¡å¼"""
        
        features = self._extract_features(contention, historical)
        
        # è§„åˆ™å¼•æ“ + æœºå™¨å­¦ä¹ æ··åˆæ¨¡å¼
        rule_based_result = self._rule_based_recognition(features)
        ml_based_result = self._ml_based_recognition(features)
        
        # èåˆç»“æœ
        pattern = self._merge_results(rule_based_result, ml_based_result)
        
        return ContentionPattern(
            type=pattern.type,
            confidence=pattern.confidence,
            characteristics=pattern.characteristics,
            root_causes=self._identify_root_causes(pattern, features)
        )
    
    def _extract_features(
        self, 
        contention: ContentionData,
        historical: List[ContentionData]
    ) -> Features:
        """æå–ç‰¹å¾å‘é‡"""
        
        return Features(
            # æ—¶é—´ç‰¹å¾
            time_of_day=contention.timestamp.hour,
            day_of_week=contention.timestamp.weekday(),
            is_business_hour=self._is_business_hour(contention.timestamp),
            
            # é¢‘ç‡ç‰¹å¾
            contention_frequency=contention.count / contention.duration,
            burst_coefficient=self._calculate_burst_coefficient(historical),
            periodicity_score=self._calculate_periodicity(historical),
            
            # å½±å“ç‰¹å¾
            affected_session_count=contention.affected_sessions,
            avg_wait_time=contention.avg_wait_time,
            max_wait_time=contention.max_wait_time,
            total_wait_time=contention.total_wait_time,
            
            # å¯¹è±¡ç‰¹å¾
            object_type=contention.object_type,
            object_size=contention.object_size,
            access_pattern=contention.access_pattern,
            
            # æŸ¥è¯¢ç‰¹å¾
            query_complexity=self._calculate_query_complexity(contention.queries),
            lock_mode_distribution=contention.lock_mode_distribution,
            transaction_size_avg=contention.avg_transaction_size
        )
    
    def _rule_based_recognition(self, features: Features) -> PatternResult:
        """åŸºäºè§„åˆ™çš„æ¨¡å¼è¯†åˆ«"""
        
        # çƒ­ç‚¹ç«äº‰ï¼šé«˜é¢‘ç‡ + é›†ä¸­åœ¨å°‘æ•°å¯¹è±¡
        if (features.contention_frequency > 10 and 
            features.avg_wait_time > 1.0):
            return PatternResult('hot_spot', confidence=0.9)
        
        # é¡ºåºé”®ç«äº‰ï¼šå‘¨æœŸæ€§ + ç‰¹å®šæ—¶é—´æ®µ
        if (features.periodicity_score > 0.7 and 
            features.access_pattern == 'sequential'):
            return PatternResult('sequential_key', confidence=0.85)
        
        # çªå‘ç«äº‰ï¼šé«˜çªå‘ç³»æ•°
        if features.burst_coefficient > 2.0:
            return PatternResult('burst', confidence=0.8)
        
        # é»˜è®¤æ¨¡å¼
        return PatternResult('unknown', confidence=0.5)
```

### 5.3 ç¼“å­˜ç­–ç•¥è®¾è®¡

```python
class LockAnalysisCache:
    """é”åˆ†æç¼“å­˜ç®¡ç†å™¨ - å¤šçº§ç¼“å­˜ç­–ç•¥"""
    
    def __init__(self, redis_client: Redis):
        self.redis = redis_client
        self.local_cache = TTLCache(maxsize=1000, ttl=60)  # æœ¬åœ°1åˆ†é’Ÿç¼“å­˜
        
        # ä¸åŒæ•°æ®ç±»å‹çš„TTLé…ç½®
        self.ttl_config = {
            'realtime': 10,      # å®æ—¶æ•°æ®: 10ç§’
            'analysis': 300,     # åˆ†æç»“æœ: 5åˆ†é’Ÿ
            'historical': 3600,  # å†å²æ•°æ®: 1å°æ—¶
            'statistics': 1800   # ç»Ÿè®¡æ•°æ®: 30åˆ†é’Ÿ
        }
    
    async def get_or_compute(
        self,
        key: str,
        compute_func: Callable,
        data_type: str = 'analysis',
        force_refresh: bool = False
    ) -> Any:
        """è·å–ç¼“å­˜æˆ–è®¡ç®—æ–°å€¼"""
        
        # 1. æ£€æŸ¥æœ¬åœ°ç¼“å­˜
        if not force_refresh and key in self.local_cache:
            logger.debug(f"Cache hit (local): {key}")
            return self.local_cache[key]
        
        # 2. æ£€æŸ¥Redisç¼“å­˜
        if not force_refresh:
            cached_value = await self.redis.get(key)
            if cached_value:
                logger.debug(f"Cache hit (redis): {key}")
                value = pickle.loads(cached_value)
                self.local_cache[key] = value  # å›å¡«æœ¬åœ°ç¼“å­˜
                return value
        
        # 3. è®¡ç®—æ–°å€¼
        logger.debug(f"Cache miss, computing: {key}")
        value = await compute_func()
        
        # 4. å†™å…¥ç¼“å­˜
        await self._set_cache(key, value, data_type)
        
        return value
    
    async def _set_cache(self, key: str, value: Any, data_type: str):
        """è®¾ç½®å¤šçº§ç¼“å­˜"""
        # æœ¬åœ°ç¼“å­˜
        self.local_cache[key] = value
        
        # Redisç¼“å­˜
        ttl = self.ttl_config.get(data_type, 300)
        serialized = pickle.dumps(value)
        await self.redis.setex(key, ttl, serialized)
    
    async def invalidate(self, pattern: str):
        """å¤±æ•ˆæŒ‡å®šæ¨¡å¼çš„ç¼“å­˜"""
        # æ¸…é™¤æœ¬åœ°ç¼“å­˜
        keys_to_remove = [k for k in self.local_cache.keys() if fnmatch(k, pattern)]
        for key in keys_to_remove:
            del self.local_cache[key]
        
        # æ¸…é™¤Redisç¼“å­˜
        async for key in self.redis.scan_iter(match=pattern):
            await self.redis.delete(key)
```

### 5.4 å¼‚æ­¥ä¼˜åŒ–æ–¹æ¡ˆ

```python
class AsyncLockAnalysisOrchestrator:
    """å¼‚æ­¥é”åˆ†æç¼–æ’å™¨"""
    
    def __init__(
        self,
        collector: ILockDataCollector,
        engine: LockAnalysisEngine,
        advisor: OptimizationAdvisor,
        cache: LockAnalysisCache
    ):
        self.collector = collector
        self.engine = engine
        self.advisor = advisor
        self.cache = cache
    
    async def analyze_comprehensive(
        self,
        database_id: int,
        options: AnalysisOptions
    ) -> ComprehensiveAnalysisResult:
        """ç»¼åˆåˆ†æ - å¹¶è¡Œæ‰§è¡Œå¤šä¸ªåˆ†æä»»åŠ¡"""
        
        cache_key = f"analysis:comprehensive:{database_id}"
        
        async def compute():
            # å¹¶è¡Œæ‰§è¡Œæ•°æ®é‡‡é›†
            locks, wait_chains, statistics = await asyncio.gather(
                self.collector.collect_current_locks(),
                self.collector.collect_blocking_tree(),
                self.collector.collect_lock_statistics(options.duration),
                return_exceptions=True
            )
            
            # å¤„ç†é‡‡é›†é”™è¯¯
            locks = locks if not isinstance(locks, Exception) else []
            wait_chains = wait_chains if not isinstance(wait_chains, Exception) else []
            statistics = statistics if not isinstance(statistics, Exception) else LockStatistics()
            
            # æ„å»ºåˆ†ææ•°æ®
            analysis_data = LockData(
                locks=locks,
                wait_chains=wait_chains,
                statistics=statistics
            )
            
            # å¹¶è¡Œæ‰§è¡Œå¤šä¸ªåˆ†æå™¨
            analysis_tasks = [
                self.engine.analyze_wait_chains(analysis_data),
                self.engine.analyze_contentions(analysis_data),
                self.engine.analyze_performance_impact(analysis_data),
                self.engine.calculate_health_score(analysis_data)
            ]
            
            results = await asyncio.gather(*analysis_tasks, return_exceptions=True)
            
            # åˆå¹¶ç»“æœ
            analysis_result = AnalysisResult(
                wait_chain_analysis=results[0],
                contention_analysis=results[1],
                performance_impact=results[2],
                health_score=results[3]
            )
            
            # ç”Ÿæˆä¼˜åŒ–å»ºè®®
            advice = await self.advisor.generate_advice(
                analysis_result,
                AnalysisContext(database_id=database_id)
            )
            
            return ComprehensiveAnalysisResult(
                analysis=analysis_result,
                advice=advice,
                timestamp=datetime.now()
            )
        
        # ä½¿ç”¨ç¼“å­˜
        return await self.cache.get_or_compute(
            cache_key,
            compute,
            data_type='analysis',
            force_refresh=options.force_refresh
        )
```

### 5.5 ç›‘æ§å’Œå‘Šè­¦è®¾è®¡

```python
class LockMonitoringService:
    """é”ç›‘æ§æœåŠ¡"""
    
    def __init__(
        self,
        collector: ILockDataCollector,
        alert_manager: AlertManager,
        metrics_store: MetricsStore
    ):
        self.collector = collector
        self.alert_manager = alert_manager
        self.metrics_store = metrics_store
        self.monitoring_tasks: Dict[int, asyncio.Task] = {}
    
    async def start_monitoring(
        self,
        database_id: int,
        config: MonitoringConfig
    ):
        """å¯åŠ¨ç›‘æ§ä»»åŠ¡"""
        
        if database_id in self.monitoring_tasks:
            logger.warning(f"Monitoring already running for database {database_id}")
            return
        
        task = asyncio.create_task(
            self._monitoring_loop(database_id, config)
        )
        self.monitoring_tasks[database_id] = task
        
        logger.info(f"Started monitoring for database {database_id}")
    
    async def _monitoring_loop(
        self,
        database_id: int,
        config: MonitoringConfig
    ):
        """ç›‘æ§å¾ªç¯"""
        
        while True:
            try:
                # é‡‡é›†æŒ‡æ ‡
                metrics = await self._collect_metrics(database_id)
                
                # å­˜å‚¨æŒ‡æ ‡
                await self.metrics_store.store(database_id, metrics)
                
                # æ£€æŸ¥å‘Šè­¦è§„åˆ™
                await self._check_alert_rules(database_id, metrics, config)
                
                # ç­‰å¾…ä¸‹ä¸€æ¬¡é‡‡é›†
                await asyncio.sleep(config.interval)
                
            except asyncio.CancelledError:
                logger.info(f"Monitoring cancelled for database {database_id}")
                break
            except Exception as e:
                logger.error(f"Error in monitoring loop: {e}", exc_info=True)
                await asyncio.sleep(config.interval)
    
    async def _check_alert_rules(
        self,
        database_id: int,
        metrics: LockMetrics,
        config: MonitoringConfig
    ):
        """æ£€æŸ¥å‘Šè­¦è§„åˆ™"""
        
        alerts = []
        
        # 1. ç­‰å¾…æ—¶é—´å‘Šè­¦
        if metrics.p99_wait_time > config.thresholds.wait_time:
            alerts.append(Alert(
                level='warning',
                type='high_wait_time',
                message=f"P99 wait time {metrics.p99_wait_time}s exceeds threshold",
                database_id=database_id,
                metrics=metrics
            ))
        
        # 2. æ­»é”å‘Šè­¦
        if metrics.deadlock_count > config.thresholds.deadlock_count:
            alerts.append(Alert(
                level='critical',
                type='deadlock',
                message=f"Deadlock count {metrics.deadlock_count} exceeds threshold",
                database_id=database_id,
                metrics=metrics
            ))
        
        # 3. é•¿æ—¶é—´é˜»å¡å‘Šè­¦
        if metrics.max_chain_length > config.thresholds.chain_length:
            alerts.append(Alert(
                level='warning',
                type='long_blocking_chain',
                message=f"Blocking chain length {metrics.max_chain_length} exceeds threshold",
                database_id=database_id,
                metrics=metrics
            ))
        
        # å‘é€å‘Šè­¦
        for alert in alerts:
            await self.alert_manager.send_alert(alert)
```

### 5.6 æµ‹è¯•ç­–ç•¥

```python
# å•å…ƒæµ‹è¯•ç¤ºä¾‹
class TestPostgreSQLLockCollector:
    """PostgreSQLé”é‡‡é›†å™¨å•å…ƒæµ‹è¯•"""
    
    @pytest.fixture
    async def collector(self, pg_pool):
        return PostgreSQLLockCollector(pg_pool)
    
    @pytest.mark.asyncio
    async def test_collect_current_locks(self, collector):
        """æµ‹è¯•é‡‡é›†å½“å‰é”"""
        locks = await collector.collect_current_locks()
        
        assert isinstance(locks, list)
        for lock in locks:
            assert isinstance(lock, LockSnapshot)
            assert lock.lock_type in ['relation', 'transactionid', 'tuple']
            assert lock.mode in ['AccessShareLock', 'RowShareLock', 'ExclusiveLock']
    
    @pytest.mark.asyncio
    async def test_collect_blocking_tree_empty(self, collector):
        """æµ‹è¯•æ— é˜»å¡æƒ…å†µ"""
        tree = await collector.collect_blocking_tree()
        assert tree == []
    
    @pytest.mark.asyncio
    async def test_collect_blocking_tree_with_blocking(
        self, 
        collector,
        create_blocking_scenario
    ):
        """æµ‹è¯•æœ‰é˜»å¡æƒ…å†µ"""
        # åˆ›å»ºé˜»å¡åœºæ™¯
        await create_blocking_scenario()
        
        tree = await collector.collect_blocking_tree()
        
        assert len(tree) > 0
        assert tree[0].level == 1
        assert tree[0].blocked_pid != tree[0].blocking_pid

# é›†æˆæµ‹è¯•ç¤ºä¾‹
class TestLockAnalysisIntegration:
    """é”åˆ†æé›†æˆæµ‹è¯•"""
    
    @pytest.mark.integration
    @pytest.mark.asyncio
    async def test_end_to_end_analysis(
        self,
        test_database,
        orchestrator
    ):
        """ç«¯åˆ°ç«¯åˆ†ææµ‹è¯•"""
        
        # 1. åˆ›å»ºæµ‹è¯•æ•°æ®å’Œé”åœºæ™¯
        await self._setup_test_scenario(test_database)
        
        # 2. æ‰§è¡Œåˆ†æ
        result = await orchestrator.analyze_comprehensive(
            database_id=test_database.id,
            options=AnalysisOptions(duration=timedelta(minutes=5))
        )
        
        # 3. éªŒè¯ç»“æœ
        assert result.analysis.health_score >= 0
        assert result.analysis.health_score <= 100
        assert len(result.analysis.wait_chain_analysis.chains) > 0
        assert len(result.advice) > 0
        
        # 4. éªŒè¯å»ºè®®è´¨é‡
        high_priority_advice = [
            a for a in result.advice 
            if a.priority == 'high'
        ]
        assert len(high_priority_advice) > 0

# æ€§èƒ½æµ‹è¯•ç¤ºä¾‹
class TestLockAnalysisPerformance:
    """é”åˆ†ææ€§èƒ½æµ‹è¯•"""
    
    @pytest.mark.benchmark
    @pytest.mark.asyncio
    async def test_analysis_latency(
        self,
        orchestrator,
        benchmark
    ):
        """æµ‹è¯•åˆ†æå»¶è¿Ÿ"""
        
        async def run_analysis():
            return await orchestrator.analyze_comprehensive(
                database_id=1,
                options=AnalysisOptions()
            )
        
        # è¿è¡ŒåŸºå‡†æµ‹è¯•
        result = await benchmark(run_analysis)
        
        # éªŒè¯æ€§èƒ½æŒ‡æ ‡
        assert benchmark.stats['mean'] < 0.1  # å¹³å‡<100ms
        assert benchmark.stats['p99'] < 0.2   # P99<200ms
```

---

## 6. å®æ–½è®¡åˆ’

### 6.1 åˆ†é˜¶æ®µå®æ–½è·¯çº¿å›¾

#### Phase 1: åŸºç¡€é‡æ„ (Week 1-2)
**ç›®æ ‡**: å»ºç«‹æ–°æ¶æ„åŸºç¡€ï¼Œå®Œæˆæ ¸å¿ƒæ¥å£å®šä¹‰

**ä»»åŠ¡æ¸…å•**:
- [ ] å®šä¹‰æ ¸å¿ƒæ¥å£ (`ILockDataCollector`, `IAnalyzer`, `IOptimizationStrategy`)
- [ ] å®ç°åŸºç¡€å·¥å‚ç±»å’Œæ³¨å†Œæœºåˆ¶
- [ ] é‡æ„æ•°æ®æ¨¡å‹ï¼Œæ·»åŠ å¿…è¦çš„ç´¢å¼•
- [ ] è®¾ç½®Redisç¼“å­˜åŸºç¡€è®¾æ–½
- [ ] é…ç½®æ—¥å¿—å’Œç›‘æ§æ¡†æ¶

**äº§å‡ºç‰©**:
- æ ¸å¿ƒæ¥å£æ–‡æ¡£
- åŸºç¡€æ¶æ„ä»£ç 
- å•å…ƒæµ‹è¯•æ¡†æ¶

#### Phase 2: æ•°æ®é‡‡é›†å¢å¼º (Week 3-4)
**ç›®æ ‡**: å®ç°çœŸå®æ•°æ®é‡‡é›†ï¼Œæ›¿æ¢Mockæ•°æ®

**ä»»åŠ¡æ¸…å•**:
- [ ] å®ç° `PostgreSQLLockCollector` å®Œæ•´åŠŸèƒ½
- [ ] å®ç° `MySQLLockCollector` å®Œæ•´åŠŸèƒ½
- [ ] å®ç° `OceanBaseLockCollector` åŸºç¡€åŠŸèƒ½
- [ ] æ·»åŠ è¿æ¥æ± ç®¡ç†
- [ ] å®ç°æ•°æ®è§£æå’Œæ ‡å‡†åŒ–
- [ ] ç¼–å†™é‡‡é›†å™¨å•å…ƒæµ‹è¯•

**äº§å‡ºç‰©**:
- ä¸‰ç§æ•°æ®åº“çš„é‡‡é›†å™¨å®ç°
- é‡‡é›†å™¨æµ‹è¯•ç”¨ä¾‹
- æ€§èƒ½åŸºå‡†æµ‹è¯•æŠ¥å‘Š

#### Phase 3: åˆ†æå¼•æ“ä¼˜åŒ– (Week 5-6)
**ç›®æ ‡**: å®ç°æ™ºèƒ½åˆ†æç®—æ³•

**ä»»åŠ¡æ¸…å•**:
- [ ] å®ç° `LockHealthScorer` å¥åº·è¯„åˆ†ç®—æ³•
- [ ] å®ç° `ContentionPatternRecognizer` æ¨¡å¼è¯†åˆ«
- [ ] å®ç° `WaitChainAnalyzer` ç­‰å¾…é“¾åˆ†æ
- [ ] å®ç° `DeadlockAnalyzer` æ­»é”åˆ†æ
- [ ] æ·»åŠ æœºå™¨å­¦ä¹ æ¨¡å‹ï¼ˆå¯é€‰ï¼‰
- [ ] ç¼–å†™åˆ†æå¼•æ“æµ‹è¯•

**äº§å‡ºç‰©**:
- å®Œæ•´çš„åˆ†æå¼•æ“
- ç®—æ³•æ–‡æ¡£å’Œæ€§èƒ½æŠ¥å‘Š
- æµ‹è¯•è¦†ç›–ç‡æŠ¥å‘Š

#### Phase 4: ä¼˜åŒ–å»ºè®®ç”Ÿæˆ (Week 7-8)
**ç›®æ ‡**: å®ç°æ™ºèƒ½ä¼˜åŒ–å»ºè®®ç³»ç»Ÿ

**ä»»åŠ¡æ¸…å•**:
- [ ] å®ç°å„ç§ä¼˜åŒ–ç­–ç•¥ç±»
- [ ] å®ç°å»ºè®®ä¼˜å…ˆçº§æ’åºç®—æ³•
- [ ] å®ç°SQLè„šæœ¬ç”Ÿæˆå™¨
- [ ] æ·»åŠ å»ºè®®æ•ˆæœé¢„ä¼°åŠŸèƒ½
- [ ] å®ç°å»ºè®®æ‰§è¡Œå’Œå›æ»š
- [ ] ç¼–å†™å»ºè®®ç”Ÿæˆæµ‹è¯•

**äº§å‡ºç‰©**:
- ä¼˜åŒ–å»ºè®®ç”Ÿæˆå™¨
- SQLè„šæœ¬æ¨¡æ¿åº“
- å»ºè®®æ•ˆæœè¯„ä¼°æŠ¥å‘Š

#### Phase 5: ç¼“å­˜å’Œæ€§èƒ½ä¼˜åŒ– (Week 9)
**ç›®æ ‡**: ä¼˜åŒ–ç³»ç»Ÿæ€§èƒ½ï¼Œå®ç°ç¼“å­˜ç­–ç•¥

**ä»»åŠ¡æ¸…å•**:
- [ ] å®ç°å¤šçº§ç¼“å­˜æœºåˆ¶
- [ ] ä¼˜åŒ–æ•°æ®åº“æŸ¥è¯¢ï¼ˆæ·»åŠ ç´¢å¼•ã€æ‰¹é‡æ“ä½œï¼‰
- [ ] å®ç°å¼‚æ­¥å¹¶å‘å¤„ç†
- [ ] æ·»åŠ æ€§èƒ½ç›‘æ§å’Œè¿½è¸ª
- [ ] è¿›è¡Œå‹åŠ›æµ‹è¯•å’Œä¼˜åŒ–

**äº§å‡ºç‰©**:
- ç¼“å­˜å®ç°å’Œé…ç½®
- æ€§èƒ½ä¼˜åŒ–æŠ¥å‘Š
- å‹åŠ›æµ‹è¯•ç»“æœ

#### Phase 6: ç›‘æ§å’Œå‘Šè­¦ (Week 10)
**ç›®æ ‡**: å®ç°å®æ—¶ç›‘æ§å’Œå‘Šè­¦åŠŸèƒ½

**ä»»åŠ¡æ¸…å•**:
- [ ] å®ç° `LockMonitoringService`
- [ ] å®ç° `AlertManager`
- [ ] é…ç½®å‘Šè­¦è§„åˆ™å¼•æ“
- [ ] é›†æˆé€šçŸ¥æ¸ é“ï¼ˆé‚®ä»¶ã€Slackç­‰ï¼‰
- [ ] å®ç°ç›‘æ§ä»ªè¡¨æ¿
- [ ] ç¼–å†™ç›‘æ§å’Œå‘Šè­¦æµ‹è¯•

**äº§å‡ºç‰©**:
- ç›‘æ§æœåŠ¡å®ç°
- å‘Šè­¦è§„åˆ™é…ç½®
- ç›‘æ§æ–‡æ¡£

#### Phase 7: æµ‹è¯•å’Œæ–‡æ¡£ (Week 11)
**ç›®æ ‡**: å®Œå–„æµ‹è¯•è¦†ç›–ï¼Œç¼–å†™æ–‡æ¡£

**ä»»åŠ¡æ¸…å•**:
- [ ] è¡¥å……å•å…ƒæµ‹è¯•ï¼Œè¾¾åˆ°80%è¦†ç›–ç‡
- [ ] ç¼–å†™é›†æˆæµ‹è¯•
- [ ] ç¼–å†™æ€§èƒ½æµ‹è¯•
- [ ] ç¼–å†™APIæ–‡æ¡£
- [ ] ç¼–å†™ç”¨æˆ·æ‰‹å†Œ
- [ ] ç¼–å†™è¿ç»´æ–‡æ¡£

**äº§å‡ºç‰©**:
- å®Œæ•´æµ‹è¯•å¥—ä»¶
- APIæ–‡æ¡£
- ç”¨æˆ·å’Œè¿ç»´æ–‡æ¡£

#### Phase 8: ä¸Šçº¿å’ŒéªŒè¯ (Week 12)
**ç›®æ ‡**: ç°åº¦å‘å¸ƒï¼Œç”Ÿäº§éªŒè¯

**ä»»åŠ¡æ¸…å•**:
- [ ] å‡†å¤‡ç°åº¦å‘å¸ƒè®¡åˆ’
- [ ] åœ¨æµ‹è¯•ç¯å¢ƒå®Œæ•´éªŒè¯
- [ ] ç°åº¦å‘å¸ƒåˆ°ç”Ÿäº§ç¯å¢ƒ
- [ ] ç›‘æ§ç”Ÿäº§æŒ‡æ ‡
- [ ] æ”¶é›†ç”¨æˆ·åé¦ˆ
- [ ] ä¼˜åŒ–å’Œä¿®å¤é—®é¢˜

**äº§å‡ºç‰©**:
- å‘å¸ƒæŠ¥å‘Š
- ç”Ÿäº§ç›‘æ§ä»ªè¡¨æ¿
- é—®é¢˜ä¿®å¤è®°å½•

### 6.2 èµ„æºéœ€æ±‚

#### äººåŠ›èµ„æº
- **åç«¯å¼€å‘**: 2äºº Ã— 12å‘¨ = 24äººå‘¨
- **å‰ç«¯å¼€å‘**: 1äºº Ã— 6å‘¨ = 6äººå‘¨
- **æµ‹è¯•å·¥ç¨‹å¸ˆ**: 1äºº Ã— 4å‘¨ = 4äººå‘¨
- **DBAé¡¾é—®**: 1äºº Ã— 2å‘¨ = 2äººå‘¨

#### åŸºç¡€è®¾æ–½
- **å¼€å‘ç¯å¢ƒ**: 3å°è™šæ‹Ÿæœºï¼ˆåç«¯ã€æ•°æ®åº“ã€Redisï¼‰
- **æµ‹è¯•ç¯å¢ƒ**: 3å°è™šæ‹Ÿæœº + æµ‹è¯•æ•°æ®åº“é›†ç¾¤
- **ç”Ÿäº§ç¯å¢ƒ**: æ ¹æ®å®é™…éœ€æ±‚é…ç½®

### 6.3 é£é™©ç®¡ç†

| é£é™© | å½±å“ | æ¦‚ç‡ | ç¼“è§£æªæ–½ |
|------|------|------|----------|
| æ•°æ®åº“æƒé™ä¸è¶³ | é«˜ | ä¸­ | æå‰æ¢³ç†æƒé™éœ€æ±‚ï¼Œå‡†å¤‡é™çº§æ–¹æ¡ˆ |
| æ€§èƒ½ç›®æ ‡æœªè¾¾æˆ | ä¸­ | ä¸­ | æŒç»­æ€§èƒ½æµ‹è¯•ï¼Œé¢„ç•™ä¼˜åŒ–æ—¶é—´ |
| å…¼å®¹æ€§é—®é¢˜ | ä¸­ | ä½ | å¤šç‰ˆæœ¬æµ‹è¯•ï¼Œå……åˆ†çš„é›†æˆæµ‹è¯• |
| æ•°æ®é‡‡é›†å½±å“ç”Ÿäº§ | é«˜ | ä½ | é™åˆ¶æŸ¥è¯¢é¢‘ç‡ï¼Œä½¿ç”¨åªè¯»å‰¯æœ¬ |
| å›¢é˜ŸæŠ€èƒ½å·®è· | ä¸­ | ä¸­ | æä¾›åŸ¹è®­ï¼Œç»“å¯¹ç¼–ç¨‹ |

---

## 7. é¢„æœŸæ”¶ç›Š

### 7.1 æŠ€æœ¯æ”¶ç›Š

#### æ€§èƒ½æå‡
- **APIå“åº”æ—¶é—´**: ä» 500ms â†’ 100ms (80%æå‡)
- **æ•°æ®é‡‡é›†å¼€é”€**: <1% CPU/Memory
- **ç¼“å­˜å‘½ä¸­ç‡**: >80%
- **å¹¶å‘èƒ½åŠ›**: æ”¯æŒ 1000+ TPS

#### å¯é æ€§æå‡
- **æµ‹è¯•è¦†ç›–ç‡**: ä» 0% â†’ 80%
- **ç³»ç»Ÿå¯ç”¨æ€§**: 99.9%
- **MTTR**: <5åˆ†é’Ÿ
- **æ•°æ®å‡†ç¡®æ€§**: >95%

#### å¯ç»´æŠ¤æ€§æå‡
- **ä»£ç å¤æ‚åº¦**: é™ä½ 30%
- **æ–°åŠŸèƒ½å¼€å‘é€Ÿåº¦**: æå‡ 50%
- **Bugä¿®å¤æ—¶é—´**: ç¼©çŸ­ 40%
- **æ–‡æ¡£å®Œæ•´æ€§**: 100%

### 7.2 ä¸šåŠ¡æ”¶ç›Š

#### è¿ç»´æ•ˆç‡æå‡
- **é—®é¢˜è¯Šæ–­æ—¶é—´**: ä» 2å°æ—¶ â†’ 15åˆ†é’Ÿ
- **ä¼˜åŒ–æ–¹æ¡ˆåˆ¶å®š**: ä» 1å¤© â†’ 1å°æ—¶
- **äººå·¥ä»‹å…¥å‡å°‘**: å‡å°‘ 60%

#### æˆæœ¬èŠ‚çº¦
- **äººåŠ›æˆæœ¬**: æ¯æœˆèŠ‚çº¦ 40äººæ—¶
- **ç³»ç»Ÿèµ„æº**: ä¼˜åŒ–åèŠ‚çº¦ 20% èµ„æº
- **æ•…éšœæŸå¤±**: å‡å°‘ 80% æ•…éšœæ—¶é—´

#### ç”¨æˆ·ä½“éªŒæå‡
- **åŠŸèƒ½å®Œæ•´æ€§**: æ”¯æŒå…¨ç”Ÿå‘½å‘¨æœŸé”ç®¡ç†
- **æ˜“ç”¨æ€§**: æ™ºèƒ½åŒ–å»ºè®®ï¼Œä¸€é”®ä¼˜åŒ–
- **å¯è§†åŒ–**: ç›´è§‚çš„ç›‘æ§ä»ªè¡¨æ¿

### 7.3 é•¿æœŸä»·å€¼

#### æŠ€æœ¯å‚¨å¤‡
- å»ºç«‹ä¼ä¸šçº§æ•°æ®åº“æ€§èƒ½åˆ†æå¹³å°
- ç§¯ç´¯é”åˆ†æå’Œä¼˜åŒ–ç»éªŒ
- å½¢æˆå¯å¤ç”¨çš„æŠ€æœ¯ç»„ä»¶

#### äº§å“ç«äº‰åŠ›
- è¶…è¶Šå¼€æºäº§å“çš„æ™ºèƒ½åŒ–æ°´å¹³
- åª²ç¾å•†ä¸šäº§å“çš„åŠŸèƒ½å®Œæ•´æ€§
- ç‹¬ç‰¹çš„å¤šæ•°æ®åº“ç»Ÿä¸€ç®¡ç†èƒ½åŠ›

#### å›¢é˜Ÿæˆé•¿
- æå‡å›¢é˜ŸæŠ€æœ¯èƒ½åŠ›
- ç§¯ç´¯å¤æ‚ç³»ç»Ÿè®¾è®¡ç»éªŒ
- å½¢æˆæœ€ä½³å®è·µå’Œæ–¹æ³•è®º

---

## 8. é™„å½•

### 8.1 æ ¸å¿ƒä»£ç æ–‡ä»¶æ¸…å•

```
udbm-backend/app/services/lock_analysis/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ collectors/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base.py                    # åŸºç¡€é‡‡é›†å™¨æ¥å£
â”‚   â”œâ”€â”€ postgresql_collector.py   # PostgreSQLé‡‡é›†å™¨
â”‚   â”œâ”€â”€ mysql_collector.py         # MySQLé‡‡é›†å™¨
â”‚   â””â”€â”€ oceanbase_collector.py    # OceanBaseé‡‡é›†å™¨
â”œâ”€â”€ analyzers/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base.py                    # åŸºç¡€åˆ†æå™¨æ¥å£
â”‚   â”œâ”€â”€ wait_chain_analyzer.py    # ç­‰å¾…é“¾åˆ†æå™¨
â”‚   â”œâ”€â”€ contention_analyzer.py    # ç«äº‰åˆ†æå™¨
â”‚   â”œâ”€â”€ deadlock_analyzer.py      # æ­»é”åˆ†æå™¨
â”‚   â””â”€â”€ health_scorer.py          # å¥åº·è¯„åˆ†å™¨
â”œâ”€â”€ advisors/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base.py                    # åŸºç¡€å»ºè®®ç­–ç•¥æ¥å£
â”‚   â”œâ”€â”€ index_strategy.py         # ç´¢å¼•ä¼˜åŒ–ç­–ç•¥
â”‚   â”œâ”€â”€ query_strategy.py         # æŸ¥è¯¢ä¼˜åŒ–ç­–ç•¥
â”‚   â”œâ”€â”€ isolation_strategy.py    # éš”ç¦»çº§åˆ«ç­–ç•¥
â”‚   â””â”€â”€ config_strategy.py        # é…ç½®ä¼˜åŒ–ç­–ç•¥
â”œâ”€â”€ monitoring/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ monitor_service.py        # ç›‘æ§æœåŠ¡
â”‚   â”œâ”€â”€ alert_manager.py          # å‘Šè­¦ç®¡ç†å™¨
â”‚   â””â”€â”€ metrics_store.py          # æŒ‡æ ‡å­˜å‚¨
â”œâ”€â”€ cache.py                       # ç¼“å­˜ç®¡ç†
â”œâ”€â”€ orchestrator.py               # åˆ†æç¼–æ’å™¨
â””â”€â”€ models.py                      # æ•°æ®æ¨¡å‹
```

### 8.2 æ•°æ®åº“Schemaå˜æ›´

```sql
-- æ·»åŠ ç´¢å¼•ä»¥æå‡æŸ¥è¯¢æ€§èƒ½
CREATE INDEX idx_lock_events_database_time 
ON udbm.lock_events(database_id, lock_request_time DESC);

CREATE INDEX idx_lock_events_object 
ON udbm.lock_events(database_id, object_name, lock_request_time DESC);

CREATE INDEX idx_lock_wait_chains_severity 
ON udbm.lock_wait_chains(database_id, severity_level, created_at DESC);

-- æ·»åŠ åˆ†åŒºä»¥æå‡å†å²æ•°æ®æŸ¥è¯¢æ€§èƒ½ï¼ˆPostgreSQL 10+ï¼‰
ALTER TABLE udbm.lock_events 
PARTITION BY RANGE (lock_request_time);

CREATE TABLE udbm.lock_events_y2024m01 
PARTITION OF udbm.lock_events
FOR VALUES FROM ('2024-01-01') TO ('2024-02-01');

-- æ·»åŠ ç‰©åŒ–è§†å›¾ä»¥åŠ é€Ÿçƒ­ç‚¹å¯¹è±¡æŸ¥è¯¢
CREATE MATERIALIZED VIEW udbm.mv_lock_hot_objects AS
SELECT 
    database_id,
    object_name,
    COUNT(*) as contention_count,
    SUM(wait_duration) as total_wait_time,
    AVG(wait_duration) as avg_wait_time,
    MAX(wait_duration) as max_wait_time,
    COUNT(DISTINCT session_id) as affected_sessions
FROM udbm.lock_events
WHERE lock_request_time >= NOW() - INTERVAL '24 hours'
    AND wait_duration > 0
GROUP BY database_id, object_name
HAVING COUNT(*) >= 10
ORDER BY total_wait_time DESC;

-- åˆ›å»ºåˆ·æ–°ä»»åŠ¡
CREATE UNIQUE INDEX ON udbm.mv_lock_hot_objects(database_id, object_name);
REFRESH MATERIALIZED VIEW CONCURRENTLY udbm.mv_lock_hot_objects;
```

### 8.3 é…ç½®å‚æ•°è¯´æ˜

```yaml
# config/lock_analysis.yaml

# æ•°æ®é‡‡é›†é…ç½®
collection:
  # é‡‡é›†é—´éš”ï¼ˆç§’ï¼‰
  interval: 10
  # é‡‡é›†è¶…æ—¶ï¼ˆç§’ï¼‰
  timeout: 5
  # æœ€å¤§é‡è¯•æ¬¡æ•°
  max_retries: 3
  # æ‰¹é‡å¤§å°
  batch_size: 1000

# åˆ†æé…ç½®
analysis:
  # å¥åº·è¯„åˆ†æƒé‡
  health_score_weights:
    wait_time: 0.30
    contention: 0.25
    deadlock: 0.20
    blocking_chain: 0.15
    timeout: 0.10
  
  # ç«äº‰æ¨¡å¼é˜ˆå€¼
  contention_thresholds:
    hot_spot_frequency: 10  # æ¯åˆ†é’Ÿ
    hot_spot_wait_time: 1.0  # ç§’
    burst_coefficient: 2.0
    periodicity_score: 0.7

# ç¼“å­˜é…ç½®
cache:
  # Redisé…ç½®
  redis:
    host: localhost
    port: 6379
    db: 1
    password: null
  
  # TTLé…ç½®ï¼ˆç§’ï¼‰
  ttl:
    realtime: 10
    analysis: 300
    historical: 3600
    statistics: 1800
  
  # æœ¬åœ°ç¼“å­˜
  local:
    max_size: 1000
    ttl: 60

# ç›‘æ§é…ç½®
monitoring:
  # é»˜è®¤ç›‘æ§é—´éš”ï¼ˆç§’ï¼‰
  default_interval: 60
  # æŒ‡æ ‡ä¿ç•™æ—¶é—´ï¼ˆå¤©ï¼‰
  retention_days: 30
  
  # å‘Šè­¦é˜ˆå€¼
  alert_thresholds:
    wait_time_p99: 5.0  # ç§’
    deadlock_count: 5    # æ¯å°æ—¶
    chain_length: 5      # é“¾é•¿åº¦
    timeout_rate: 0.1    # 10%

# æ€§èƒ½é…ç½®
performance:
  # å¼‚æ­¥ä»»åŠ¡å¹¶å‘æ•°
  max_concurrent_tasks: 10
  # è¿æ¥æ± å¤§å°
  connection_pool_size: 10
  # æŸ¥è¯¢è¶…æ—¶ï¼ˆç§’ï¼‰
  query_timeout: 30
```

### 8.4 ç›‘æ§æŒ‡æ ‡è¯´æ˜

#### ä¸šåŠ¡æŒ‡æ ‡
- `lock_health_score`: é”å¥åº·è¯„åˆ† (0-100)
- `lock_wait_time_p99`: P99ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰
- `lock_contention_rate`: é”ç«äº‰ç‡ï¼ˆæ¬¡/åˆ†é’Ÿï¼‰
- `deadlock_count`: æ­»é”æ¬¡æ•°
- `timeout_count`: è¶…æ—¶æ¬¡æ•°
- `active_chain_count`: æ´»è·ƒç­‰å¾…é“¾æ•°é‡
- `hot_object_count`: çƒ­ç‚¹å¯¹è±¡æ•°é‡

#### ç³»ç»ŸæŒ‡æ ‡
- `collection_duration`: é‡‡é›†è€—æ—¶ï¼ˆæ¯«ç§’ï¼‰
- `analysis_duration`: åˆ†æè€—æ—¶ï¼ˆæ¯«ç§’ï¼‰
- `cache_hit_rate`: ç¼“å­˜å‘½ä¸­ç‡ï¼ˆ%ï¼‰
- `api_response_time`: APIå“åº”æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
- `error_rate`: é”™è¯¯ç‡ï¼ˆ%ï¼‰

#### èµ„æºæŒ‡æ ‡
- `cpu_usage`: CPUä½¿ç”¨ç‡ï¼ˆ%ï¼‰
- `memory_usage`: å†…å­˜ä½¿ç”¨ç‡ï¼ˆ%ï¼‰
- `connection_pool_usage`: è¿æ¥æ± ä½¿ç”¨ç‡ï¼ˆ%ï¼‰
- `redis_memory_usage`: Rediså†…å­˜ä½¿ç”¨ï¼ˆMBï¼‰

---

## æ€»ç»“

æœ¬é‡æ„æ–¹æ¡ˆåŸºäºä¸šç•Œæœ€ä½³å®è·µï¼Œé‡‡ç”¨SOLIDåŸåˆ™å’Œç»å…¸è®¾è®¡æ¨¡å¼ï¼Œæ—¨åœ¨å°†é”åˆ†ææ¨¡å—ä»å½“å‰çš„MVPçŠ¶æ€å‡çº§ä¸ºä¼ä¸šçº§çš„ç”Ÿäº§å°±ç»ªç³»ç»Ÿã€‚

**æ ¸å¿ƒæ”¹è¿›**:
1. ä»Mockæ•°æ®åˆ°çœŸå®æ•°æ®é‡‡é›†
2. ä»å•ä¸€ç±»åˆ°åˆ†å±‚æ¶æ„
3. ä»åŒæ­¥åˆ°å¼‚æ­¥é«˜æ€§èƒ½
4. ä»ç®€å•è§„åˆ™åˆ°æ™ºèƒ½ç®—æ³•
5. ä»æ— æµ‹è¯•åˆ°é«˜è¦†ç›–ç‡

**å®æ–½å»ºè®®**:
- é‡‡ç”¨åˆ†é˜¶æ®µè¿­ä»£æ–¹å¼ï¼Œæ¯ä¸ªé˜¶æ®µéƒ½æœ‰æ˜ç¡®çš„äº¤ä»˜ç‰©
- é‡è§†æµ‹è¯•å’Œæ–‡æ¡£ï¼Œç¡®ä¿ä»£ç è´¨é‡å’Œå¯ç»´æŠ¤æ€§
- æŒç»­ç›‘æ§æ€§èƒ½æŒ‡æ ‡ï¼ŒåŠæ—¶ä¼˜åŒ–ç“¶é¢ˆ
- æ”¶é›†ç”¨æˆ·åé¦ˆï¼Œå¿«é€Ÿè¿­ä»£æ”¹è¿›

é€šè¿‡æœ¬æ¬¡é‡æ„ï¼Œé”åˆ†ææ¨¡å—å°†æˆä¸ºUDBMå¹³å°çš„æ ¸å¿ƒç«äº‰åŠ›ä¹‹ä¸€ï¼Œä¸ºç”¨æˆ·æä¾›ä¸“ä¸šã€æ™ºèƒ½ã€é«˜æ•ˆçš„æ•°æ®åº“é”æ€§èƒ½ç®¡ç†èƒ½åŠ›ã€‚